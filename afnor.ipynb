{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "import glob\n",
    "from itertools import chain\n",
    "import warnings\n",
    "from scrapers.scrapepool import ScrapePool\n",
    "from scrapers.afnor import AfnorScraper\n",
    "from secrets import users, passwords\n",
    "\n",
    "# disable warnings, because pandas throws warnings for mixed type columns all the time..\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = '.\\\\data\\\\afnor'\n",
    "write_dir = 'out'\n",
    "file_found = 'found.csv'\n",
    "file_extended = 'perinorm.csv'\n",
    "col_found_id = 'Official identifier'\n",
    "col_extended_id = 'Dokumentnummer'\n",
    "search_for_document_families = False\n",
    "search_batch_size = 500\n",
    "num_threads = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper settings\n",
    "args = {\n",
    "    'phantomjs_exec': 'C:\\\\phantomjs.exe',\n",
    "    'save_to': '.\\\\data\\\\afnor\\\\out',\n",
    "    'user': users.afnor,\n",
    "    'password': passwords.afnor,\n",
    "    'urls': {\n",
    "        'base': \"https://sagaweb.afnor.org\",\n",
    "        'results': \"/en-US/sw/Recherche/Resultat/1/?offset=10000&selectall=false\",\n",
    "        'logout': \"/en-US/sw/Identification/Deconnexion\"\n",
    "    },\n",
    "    'states': AfnorScraper.STATES_FIND_ALL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refresh(target):\n",
    "    print(\">> refreshing...\")\n",
    "    inputs = glob.glob(path.join(workspace, write_dir, \"*.csv\"))\n",
    "    inputs.append(path.join(workspace, file_found))\n",
    "    df = pd.concat((pd.read_csv(f, sep=\";\", quotechar=\"\\\"\", encoding=\"ISO-8859-1\") for f in inputs), axis=0, ignore_index=True)\n",
    "    found = set(df[col_found_id])\n",
    "    searched_for = set([\"_\".join(\".\".join(path.basename(f).split(\".\")[:-1]).split(\"_\")[:-1]).replace('#', '/').replace('=',':') for f in glob.glob(path.join(workspace, write_dir, \"*.*\"))])\n",
    "    missing = (target - found) - searched_for\n",
    "    print(\">> target: \" + str(len(target)) + \", found: \" + str(len(found)) + \", missing: \" + str(len(missing)))\n",
    "    \n",
    "    return target, found, missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extended = pd.read_csv(path.join(workspace, file_extended), sep=\";\", quotechar=\"\\\"\", encoding=\"ISO-8859-1\")\n",
    "target = set(chain.from_iterable([ref.split(\"*\") for ref in extended[col_extended_id].tolist()]))\n",
    "# manipulate keywords to search for document families\n",
    "if search_for_document_families:\n",
    "    target = set([k.split(\"/\")[0].split(\"-\")[0] for k in target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, found, missing = refresh(target)\n",
    "\n",
    "last_missing = None\n",
    "\n",
    "# make sure we're not repeatedly looking for the same thing\n",
    "while len(missing) > 0 and last_missing != missing:\n",
    "    \n",
    "    last_missing = set(missing)\n",
    "    \n",
    "    # Always only look for first n missing standards, then refresh keyword-list.\n",
    "    # This prevent searches for standards that have already been found while looking for other standards.\n",
    "    keywords = list(missing)[:min(len(missing), search_batch_size)]\n",
    "    \n",
    "    # output\n",
    "    first_next = min(len(keywords), 3)\n",
    "    second_next = len(keywords) - first_next\n",
    "    print(\">> looking for: \" + \", \".join(keywords[:first_next]) + \" (\" + str(second_next) + \" more)\")\n",
    "    \n",
    "    # run scrapers\n",
    "    s = ScrapePool(AfnorScraper, keywords, args, chunk_size=round(len(keywords)/num_threads))\n",
    "    s.run()\n",
    "    \n",
    "    _, found, missing = refresh(target)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
